{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从kaggle上下载titanic的数据, 使用pandas读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CSV_FILE=\"./train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId    False\n",
      "Survived       False\n",
      "Pclass         False\n",
      "Name           False\n",
      "Sex            False\n",
      "Age             True\n",
      "SibSp          False\n",
      "Parch          False\n",
      "Ticket         False\n",
      "Fare           False\n",
      "Cabin           True\n",
      "Embarked        True\n",
      "dtype: bool\n",
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "\n",
      "   Parch     Ticket     Fare Cabin Embarked  \n",
      "0      0  A/5 21171   7.2500              S  \n",
      "1      0   PC 17599  71.2833   C85        C  \n",
      "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
      "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
      "      dtype='object')\n",
      "891\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv(CSV_FILE, delimiter=',', header=0)\n",
    "columns_missing = pd.isnull(df).any()  #找到有缺失值的列\n",
    "print(columns_missing)\n",
    "#添充缺失值\n",
    "df['Age'] = df['Age'].fillna(-1) #数字\n",
    "df['Cabin'] = df['Cabin'].fillna('')\n",
    "df['Embarked'] = df['Embarked'].fillna('') \n",
    "print(df.head(2))\n",
    "print(df.columns)\n",
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 拼装bulk脚本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#========================create bulk data==================\n",
    "INDEX_NAME=\"titanic2\"\n",
    "TYPE_NAME=\"passenger\"\n",
    "ID_FIELD=\"PassengerId\"\n",
    "\n",
    "bulk_data=[]\n",
    "headers = df.columns\n",
    "for index, row in df.iterrows():\n",
    "    data_dict = {}\n",
    "    for i in range(len(row)):\n",
    "        data_dict[headers[i]] = row[i]\n",
    "    op_dict = {\n",
    "        \"index\":{\n",
    "            \"_index\": INDEX_NAME,\n",
    "            \"_type\": TYPE_NAME,\n",
    "            \"_id\": data_dict[ID_FIELD]\n",
    "        }\n",
    "    }\n",
    "    bulk_data.append(op_dict)\n",
    "    bulk_data.append(data_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 向elasticsearch中写数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleting titanic2 index....\n",
      "response:{'acknowledged': True}\n",
      "creating titanic2 index...\n",
      "response:{'acknowledged': True, 'shards_acknowledged': True}\n"
     ]
    }
   ],
   "source": [
    "#====================create elasticsearch index========#\n",
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch(hosts=[{\"host\":\"192.168.18.187\", \"port\":9201}])\n",
    "if es.indices.exists(INDEX_NAME):\n",
    "    print(\"deleting %s index....\" % (INDEX_NAME))\n",
    "    res = es.indices.delete(index=INDEX_NAME)\n",
    "    print(\"response:%s\" % res)\n",
    "request = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\":1,\n",
    "        \"number_of_replicas\":0\n",
    "    }\n",
    "}\n",
    "print(\"creating %s index...\" % INDEX_NAME)\n",
    "res = es.indices.create(index=INDEX_NAME, body=request)\n",
    "print(\"response:%s\" % res)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bulking indexing....\n",
      "response:{'took': 0, 'timed_out': False, '_shards': {'total': 1, 'successful': 1, 'failed': 0}, 'hits': {'total': 891, 'max_score': 1.0, 'hits': [{'_index': 'titanic2', '_type': 'passenger', '_id': '1', '_score': 1.0, '_source': {'PassengerId': 1, 'Survived': 0, 'Pclass': 3, 'Name': 'Braund, Mr. Owen Harris', 'Sex': 'male', 'Age': 22.0, 'SibSp': 1, 'Parch': 0, 'Ticket': 'A/5 21171', 'Fare': 7.25, 'Cabin': '', 'Embarked': 'S'}}, {'_index': 'titanic2', '_type': 'passenger', '_id': '2', '_score': 1.0, '_source': {'PassengerId': 2, 'Survived': 1, 'Pclass': 1, 'Name': 'Cumings, Mrs. John Bradley (Florence Briggs Thayer)', 'Sex': 'female', 'Age': 38.0, 'SibSp': 1, 'Parch': 0, 'Ticket': 'PC 17599', 'Fare': 71.2833, 'Cabin': 'C85', 'Embarked': 'C'}}]}} \n"
     ]
    }
   ],
   "source": [
    "#===========push data============\n",
    "print(\"bulking indexing....\")\n",
    "res = es.bulk(index=INDEX_NAME, body=bulk_data, refresh=True)\n",
    "res = es.search(index=INDEX_NAME, size=2, body={\"query\":{\"match_all\":{}}})\n",
    "print(\"response:%s \" % res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取己存入elasticsearch的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891\n",
      "(10, 12)\n"
     ]
    }
   ],
   "source": [
    "from pandas.io.json import json_normalize\n",
    "\n",
    "res = es.search(index=INDEX_NAME, doc_type=TYPE_NAME, body={\"query\":{\"match_all\":{}}}) #size=1 for test, only return 10 records default\n",
    "print(res['hits']['total'])\n",
    "df = json_normalize(res['hits']['hits'])\n",
    "#filter df by column name\n",
    "df2 = df.filter(regex=\"^_source\\.\")\n",
    "#print(df2)\n",
    "#rename column name\n",
    "df3 = df2.rename(columns=lambda x:x.replace(\"_source.\", \"\"))\n",
    "#print(df3)\n",
    "df = df3\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891\n",
      "(891, 12)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Start scrolling\\nwhile(scroll_size >0):\\n    print \"Scrolling...\"\\n    page = es.scroll(scroll_id = sid, scroll =\\'2m\\')\\n    # Update the scroll ID\\n    sid = page[\\'_scroll_id\\']\\n    # Get the number of results that we returned in the last scroll\\n    scroll_size = len(page[\\'hits\\'][\\'hits\\'])\\n    print \"scroll size: \"+ str(scroll_size)\\n    # Do something with the obtained page\\n'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#res = es.search(index=INDEX_NAME, doc_type=TYPE_NAME, scroll=\"1m\", search_type=\"query_then_fetch\", body={\"query\":{\"match_all\":{}}})\n",
    "res = es.search(index=INDEX_NAME, doc_type=TYPE_NAME, size=891, body={\"query\":{\"match_all\":{}}})  #todo:how to scroll\n",
    "print(res['hits']['total'])\n",
    "df = json_normalize(res['hits']['hits'])\n",
    "df2 = df.filter(regex=\"^_source\\.\")\n",
    "#print(df2)\n",
    "#rename column name\n",
    "df3 = df2.rename(columns=lambda x:x.replace(\"_source.\", \"\"))\n",
    "#print(df3)\n",
    "df = df3\n",
    "print(df.shape)\n",
    "\n",
    "'''\n",
    "# Start scrolling\n",
    "while(scroll_size >0):\n",
    "    print \"Scrolling...\"\n",
    "    page = es.scroll(scroll_id = sid, scroll ='2m')\n",
    "    # Update the scroll ID\n",
    "    sid = page['_scroll_id']\n",
    "    # Get the number of results that we returned in the last scroll\n",
    "    scroll_size = len(page['hits']['hits'])\n",
    "    print \"scroll size: \"+ str(scroll_size)\n",
    "    # Do something with the obtained page\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC()\n",
    "clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[145  21]\n",
      " [ 78  24]]\n",
      "0.630597014925\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      0.87      0.75       166\n",
      "          1       0.53      0.24      0.33       102\n",
      "\n",
      "avg / total       0.61      0.63      0.59       268\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import *\n",
    "\n",
    "predict = clf.predict(valid_x)\n",
    "cm = confusion_matrix(valid_y, predict)\n",
    "print(cm)\n",
    "print(accuracy_score(valid_y, predict))\n",
    "print(classification_report(valid_y, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
