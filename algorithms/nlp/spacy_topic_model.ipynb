{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ElasticSearchClass' from 'E:\\\\my_study_place\\\\python\\\\jupyter\\\\spacy\\\\ElasticSearchClass.py'>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin \n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stopwords \n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "import hdbscan\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "import string\n",
    "import time\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [16,9]\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import ElasticSearchClass\n",
    "import importlib\n",
    "importlib.reload(ElasticSearchClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load20NewsGroups():\n",
    "    # #############################################################################\n",
    "    # Load some categories from the training set\n",
    "    categories = [\n",
    "        'alt.atheism',\n",
    "        'talk.religion.misc',\n",
    "        'comp.graphics',\n",
    "        'sci.space',\n",
    "     ]\n",
    "    dataset = fetch_20newsgroups(subset='all', categories=categories,\n",
    "                             shuffle=True, random_state=42)\n",
    "    print(\"%d documents\" % len(dataset.data))\n",
    "    print(\"%d categories\" % len(dataset.target_names))\n",
    "    return dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object scan at 0x0000023C1655B8E0>\n",
      "Retrieved 1092 records in 0.2993195056915283 Seconds\n"
     ]
    }
   ],
   "source": [
    "def loadStackoverflowFromES():\n",
    "    esUtil = ElasticSearchClass.ElasticSearchClass(\"192.168.18.187\", 9201)\n",
    "    dsl = '''\n",
    "    {\n",
    "    \"_source\":[\"title\", \"body\"],\n",
    "    \"query\":{\n",
    "        \"bool\":{\n",
    "            \"must\":{\n",
    "                \"match\":{\"posttypeid\":1}}\n",
    "            }\n",
    "        },\n",
    "    \"size\":1\n",
    "    }\n",
    "    '''\n",
    "    res = esUtil.search(indexName=\"stackoverflow\", body=dsl)\n",
    "    for doc in res['hits']['hits']:\n",
    "         print(\"%s) %s\" % (doc['_id'], doc['_source']))\n",
    "    \n",
    "def iterLoadStackoverflowFromES():\n",
    "    esUtil = ElasticSearchClass.ElasticSearchClass(\"192.168.18.187\", 9201)\n",
    "    dsl = '''\n",
    "    {\n",
    "    \"_source\":[\"body\"],\n",
    "    \"query\":{\n",
    "        \"bool\":{\n",
    "            \"must\":{\n",
    "                \"match\":{\"posttypeid\":1}}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    '''\n",
    "    res = esUtil.scrollSearch(indexName=\"stackoverflow\", body=dsl)\n",
    "    print(res)\n",
    "    count = 0\n",
    "    data = []\n",
    "    for doc in res:\n",
    "        if count > 10000:\n",
    "            break\n",
    "        count += 1\n",
    "        data.append([doc['_id'], doc['_source']['body']])\n",
    "        #print(doc['_id'], doc['_source']['body'])\n",
    "    return data\n",
    "            \n",
    "#loadStackoverflowFromES()\n",
    "start_time = time.time()\n",
    "data = iterLoadStackoverflowFromES()\n",
    "end_time = time.time()\n",
    "print(\"Retrieved {} records in {} Seconds\".format(len(data), end_time - start_time))\n",
    "#Retrieved 101 records in 1.3270199298858643 Seconds\n",
    "#10001 records in 246.03912162780762 Seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########create data clearner\n",
    "\n",
    "#Custom transformer using spaCy \n",
    "class CleanTextTransformer(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        return [cleanText(text) for text in X]\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "# A custom function to clean the text before sending it into the vectorizer\n",
    "def cleanText(text):\n",
    "    #remove <code></code> tag\n",
    "    bs = BeautifulSoup(text, \"html.parser\")\n",
    "    code = [s.extract() for s in bs('code')]\n",
    "    # replace other HTML symbols\n",
    "    text = bs.get_text()\n",
    "    text = text.lower()\n",
    "    # get rid of newlines\n",
    "    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    # replace @mentions\n",
    "    mentionFinder = re.compile(r\"@[a-z0-9_]{1,15}\", re.IGNORECASE)\n",
    "    text = mentionFinder.sub(\"@mention\", text)\n",
    "    # delete numbers\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "############create tokenizer\n",
    "\n",
    "#Create spacy tokenizer that parses a sentence and generates tokens\n",
    "#these can also be replaced by word vectors \n",
    "# List of symbols we don't care about\n",
    "punctuations = \" \".join(string.punctuation).split(\" \") + [\"-----\", \"---\", \"...\", \"“\", \"”\", \"'ve\", \"--\", \"//\", \"div\"]\n",
    "parser = spacy.load('en')\n",
    "def tokenizeText(sentence):\n",
    "    tokens = parser(sentence)\n",
    "    #only keep nouns\n",
    "    tokens = [tok for tok in tokens if (tok.tag_ == \"NNS\" or tok.tag == \"NN\")]\n",
    "    tokens = [tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_ for tok in tokens]\n",
    "    tokens = [tok for tok in tokens if (tok not in stopwords and tok not in punctuations)]\n",
    "    # remove large strings of whitespace\n",
    "     # remove large strings of whitespace\n",
    "    while \"\" in tokens:\n",
    "        tokens.remove(\"\")\n",
    "    while \" \" in tokens:\n",
    "        tokens.remove(\" \")\n",
    "    while \"\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\")\n",
    "    while \"\\n\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\\n\")\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########Create preprocess pipline and run\n",
    "def preProcessData(X_train, max_features=None):\n",
    "    #create vectorizer object to generate feature vectors, we will use custom spacy’s tokenizer\n",
    "    #vectorizer = TfidfVectorizer(tokenizer = tokenizeText)\n",
    "    #svd = TruncatedSVD(2)\n",
    "    #normalizer = Normalizer(copy=False)\n",
    "    #removed any word that appeared in more than 70% of documents.\n",
    "    #removed any word that appeared in less than 5 documents\n",
    "    vectorizer = CountVectorizer(tokenizer = tokenizeText, min_df=5, max_df = 0.6, max_features=max_features)\n",
    "    start_time = time.time()\n",
    "    pipe_preprocess = Pipeline([(\"cleaner\", CleanTextTransformer()),\n",
    "                 (\"vectorizer\", vectorizer)])\n",
    "    X_train_preprocess = pipe_preprocess.fit_transform(X_train)\n",
    "    end_time = time.time()\n",
    "    print(\"Preprocess done in {} Seconds\".format(end_time - start_time))\n",
    "    return X_train_preprocess, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3387 documents\n",
      "4 categories\n",
      "Preprocess done in 237.4716272354126 Seconds\n"
     ]
    }
   ],
   "source": [
    "X_train = load20NewsGroups()\n",
    "X_train_preprocess, vectorizer = preProcessData(X_train, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>What is the \"purist\" or \"correct\" way to access an object's properties from within an object method that is not a getter/setter method?</p>\n",
      "\n",
      "<p>I know that from outside of the object you should use a getter/setter, but from within would you just do:</p>\n",
      "\n",
      "<p>Java:</p>\n",
      "\n",
      "<pre><code>String property = this.property;\n",
      "</code></pre>\n",
      "\n",
      "<p>PHP:</p>\n",
      "\n",
      "<pre><code>$property = $this-&gt;property;\n",
      "</code></pre>\n",
      "\n",
      "<p>or would you do:</p>\n",
      "\n",
      "<p>Java:</p>\n",
      "\n",
      "<pre><code>String property = this.getProperty();\n",
      "</code></pre>\n",
      "\n",
      "<p>PHP:</p>\n",
      "\n",
      "<pre><code>$property = $this-&gt;getProperty();\n",
      "</code></pre>\n",
      "\n",
      "<p>Forgive me if my Java is a little off, it's been a year since I programmed in Java...</p>\n",
      "\n",
      "<p><strong>EDIT:</strong></p>\n",
      "\n",
      "<p>It seems people are assuming I am talking about private or protected variables/properties only. When I learned OO I was taught to use getters/setters for every single property even if it was public (and actually I was told never to make any variable/property public). So, I may be starting off from a false assumption from the get go. It appears that people answering this question are maybe saying that you should have public properties and that those don't need getters and setters, which goes against what I was taught, and what I was talking about, although maybe that needs to be discussed as well. That's probably a good topic for a different question though...</p>\n",
      "\n",
      "Preprocess done in 26.904396057128906 Seconds\n",
      "  (0, 163)\t2\n",
      "  (0, 178)\t1\n",
      "  (0, 180)\t2\n",
      "  (0, 151)\t1\n",
      "  (1, 177)\t1\n",
      "  (1, 109)\t2\n",
      "  (1, 127)\t3\n"
     ]
    }
   ],
   "source": [
    "#print(data[1])\n",
    "X_train = [row[1] for row in data]\n",
    "print(X_train[1])\n",
    "#print(cleanText(X_train[1]))\n",
    "X_train_preprocess, vectorizer = preProcessData(X_train, 1000)\n",
    "print(X_train_preprocess[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=20, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='online', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=100, mean_change_tol=0.001,\n",
       "             n_jobs=1, n_topics=4, perp_tol=0.1, random_state=125,\n",
       "             topic_word_prior=None, total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "n_topics = 4\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=100,\n",
    "                                learning_method='online',\n",
    "                                batch_size=20,\n",
    "                                random_state=125)\n",
    "lda.fit(X_train_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "graphic image file datum program window format object mar site bit question color keyword application tool planet user source version\n",
      "Topic #1:\n",
      "people atheist value opinion jame year idea human rule post view issue thing thought lot phig animal atom user principle\n",
      "Topic #2:\n",
      "thank year day point keyword mission communication thing satellite service datum degree problem time cost month star hour mile vnew\n",
      "Topic #3:\n",
      "people thing christian child word belief man god time book year right woman law argument religion action claim muslim question\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "print_top_words(lda, feature_names, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: if you're in the IPython notebook, pyLDAvis.show() is not the best command\n",
      "      to use. Consider using pyLDAvis.display(), or pyLDAvis.enable_notebook().\n",
      "      See more information at http://pyLDAvis.github.io/quickstart.html .\n",
      "\n",
      "You must interrupt the kernel to end this command\n",
      "\n",
      "Serving to http://127.0.0.1:8889/    [Ctrl-C to exit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [13/Mar/2018 15:48:35] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [13/Mar/2018 15:48:36] \"GET /LDAvis.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [13/Mar/2018 15:48:36] \"GET /d3.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [13/Mar/2018 15:48:36] \"GET /LDAvis.js HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "#https://github.com/bmabey/pyLDAvis/issues/69\n",
    "visData = pyLDAvis.sklearn.prepare(lda, X_train_preprocess, vectorizer, mds='mmds')\n",
    "pyLDAvis.show(visData,  ip=\"127.0.0.1\", port=8889)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
