{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: memory_profiler in d:\\program_files\\anaconda3\\lib\\site-packages\n",
      "Requirement already satisfied: psutil in d:\\program_files\\anaconda3\\lib\\site-packages (from memory_profiler)\n"
     ]
    }
   ],
   "source": [
    "#if alreasy installed, don't need to run following codes\n",
    "#!conda update conda\n",
    "#!conda config --add channels conda-forge\n",
    "#!conda install spacy -y\n",
    "#!python -m spacy download en\n",
    "\n",
    "#!conda install scikit-learn -y\n",
    "#!conda install beautifulsoup4 -y\n",
    "#!conda install elasticsearch -y\n",
    "#!pip install pyLDAvis\n",
    "\n",
    "#for error ModuleNotFoundError: No module named 'cycler' when import matplotlib\n",
    "#remember restart this notebook or using importlib.reload\n",
    "#!conda remove matplotlib -y\n",
    "#!conda remove cycler -y\n",
    "#!pip uninstall cycler -y\n",
    "#!pip uninstall matplotlib -y\n",
    "#!pip install cycler\n",
    "#!pip install matplotlib\n",
    "!pip install memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import multiprocessing\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [16,9]\n",
    "  \n",
    "from Models import DataLoadModel\n",
    "from Utils import utilTools,utilDataPreprocess,utilLDA\n",
    "import importlib\n",
    "importlib.reload(DataLoadModel)\n",
    "importlib.reload(utilTools)\n",
    "importlib.reload(utilLDA)\n",
    "importlib.reload(utilDataPreprocess)\n",
    "\n",
    "IDFILE=\"./idlist_cleaned.pkl\"\n",
    "DATAFILE_CLEANED=\"./preprocessed_data_cleaned.pkl\"\n",
    "DATAFILE_TOKENIZED=\"./preprocessed_data_tokenized.pkl\"\n",
    "DATAFILE_VECTORIZED=\"./preprocessed_data_vectorized.pkl\"\n",
    "VECTORIZERFILE = \"./vectorizer.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def saveCleanedData(idlist, data):\n",
    "    if idlist is None or data is None:\n",
    "         raise ValueError('you must input two valid objects')\n",
    "    try:\n",
    "        utilTools.pickleDump(IDFILE, idlist)\n",
    "        utilTools.pickleDump(DATAFILE_CLEANED, data)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"save failed, idfile={}, dataFile={}\".format(IDFILE, DATAFILE_CLEANED))\n",
    "        raise e\n",
    "        \n",
    "# EOFError: Ran out of input, if file size is 0\n",
    "def loadCleanedData():\n",
    "    try:\n",
    "        idlist = utilTools.pickleLoad(IDFILE)\n",
    "        data = utilTools.pickleLoad(DATAFILE_CLEANED)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"load failed, idfile={}, dataFile={}\".format(IDFILE, DATAFILE_CLEANED))\n",
    "        raise e\n",
    "        \n",
    "    return idlist, data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def saveVectorizedData(data, vectorizer):\n",
    "    if idlist is None or data is None or vectorizer is None:\n",
    "         raise ValueError('you must input two valid objects')\n",
    "    try:\n",
    "        utilTools.pickleDump(DATAFILE_VECTORIZED, data)\n",
    "        utilTools.dillDump(VECTORIZERFILE, vectorizer)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"save failed,dataFile={}, vectorizerFile={}\".format(DATAFILE_VECTORIZED, VECTORIZERFILE))\n",
    "        raise e\n",
    "        \n",
    "def loadVectorizedData():\n",
    "    try:\n",
    "        data = utilTools.pickleLoad(DATAFILE_VECTORIZED)\n",
    "        vectorizer = utilTools.dillLoad(VECTORIZERFILE)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"load failed, dataFile={}, vectorizerFile={}\".format(DATAFILE_VECTORIZED, VECTORIZERFILE))\n",
    "        raise e\n",
    "        \n",
    "    return data, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadAndCleanData(force_reload=False, n_jobs=1, force_reload_file=None, force_reload_count=-1):\n",
    "    try:\n",
    "        if force_reload:\n",
    "            raise ValueError(\"will force reload data...\")\n",
    "        start_time = time.time()\n",
    "        idlist, data = loadCleanedData()\n",
    "        end_time = time.time()\n",
    "        print(\"load {} preprocessed cleaned records in {} Seconds\".format(len(idlist), end_time - start_time))\n",
    "    except Exception as e:\n",
    "        print(\"load preprocessed data failed, Will retrieve from dataset...\")\n",
    "        start_time = time.time()\n",
    "        data = DataLoadModel.loadStackoverflowFromXML(force_reload_file, force_reload_count)\n",
    "        #data = DataLoadModel.iterLoadStackoverflowFromES(1000)\n",
    "        end_time = time.time()\n",
    "        print(\"Retrieved {} records in {} Seconds\".format(len(data), end_time - start_time))\n",
    "        \n",
    "        start_time = time.time()\n",
    "        X_train = [row[1] for row in data]\n",
    "        idlist = [row[0] for row in data]\n",
    "        cleaner = utilDataPreprocess.CleanTextTransformer(n_jobs=n_jobs)\n",
    "        data = cleaner.fit_transform(X_train)\n",
    "        try:\n",
    "            saveCleanedData(idlist, data)\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"pickleDump cleaned data Failed\")\n",
    "        end_time = time.time()\n",
    "        print(\"clean data in {} Seconds\".format(end_time - start_time))\n",
    "        \n",
    "    return idlist, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/11025005/python-sharing-a-dictionary-between-parallel-processes\n",
    "#https://stackoverflow.com/questions/5442910/python-multiprocessing-pool-map-for-multiple-arguments\n",
    "def tokenizeAndVectorieData(data_in, force_reprocess=False, n_jobs=1, min_df=1, max_df = 1.0, max_features=20000):\n",
    "    #tokenized\n",
    "    try:\n",
    "        if force_reprocess:\n",
    "            raise ValueError(\"will force reprocess, tokenizing data...\")\n",
    "        start_time = time.time()\n",
    "        tokenized_data= utilTools.pickleLoad(DATAFILE_TOKENIZED)\n",
    "        end_time = time.time()\n",
    "        print(\"load preprocessed tokenized data in {} Seconds\".format(end_time - start_time))\n",
    "    except Exception as e:\n",
    "        print(\"load preprocessed tokenized data failed, Will reprocess...\")\n",
    "        start_time = time.time()\n",
    "        pool = multiprocessing.Pool(n_jobs)\n",
    "        tokenized_data = pool.map(utilDataPreprocess.tokenizeText, data_in)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        try:\n",
    "            utilTools.pickleDump(DATAFILE_TOKENIZED, tokenized_data)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"pickleDump tokenized data Failed\")\n",
    "        end_time = time.time()\n",
    "        print(\"tokenized data in {} Seconds\".format(end_time - start_time))\n",
    "        \n",
    "    #vectorize, everytime should revectorize for the tokenized data\n",
    "    vectorized_data, vectorizer = utilDataPreprocess.countvectorizeDataWithTokens(tokenized_data, \n",
    "                                                                                  min_df=min_df, \n",
    "                                                                                  max_df=max_df,\n",
    "                                                                                  max_features=max_features)\n",
    "    try:\n",
    "        saveVectorizedData(vectorized_data, vectorizer)\n",
    "    except Exception as e:\n",
    "        print(\"pickleDump vectorized data Failed\")\n",
    "        \n",
    "    return tokenized_data, vectorized_data, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorizData(data_in, force_reprocess=False, n_jobs=1, min_df=1, max_df = 1.0, max_features=20000):\n",
    "    tokens=None\n",
    "    try:\n",
    "        if force_reprocess:\n",
    "            raise ValueError(\"will force reprocess data...\")\n",
    "        start_time = time.time()\n",
    "        data, vectorizer = loadVectorizedData()\n",
    "        end_time = time.time()\n",
    "        print(\"load preprocessed vectorized data in {} Seconds\".format(end_time - start_time))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"load preprocessed vectorized data failed, Will reprocess...\")\n",
    "        tokens, data, vectorizer= tokenizeAndVectorieData(data_in, njobs=n_jobs, min_df=min_df, max_df=max_df, max_features=max_features)\n",
    "        \n",
    "    return tokens, data, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n",
      "peak memory: 271.36 MiB, increment: 0.00 MiB\n",
      "load 10001 preprocessed cleaned records in 0.011043310165405273 Seconds\n",
      "\n",
      "\n",
      "load preprocessed vectorized data in 0.016041994094848633 Seconds\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "%load_ext memory_profiler\n",
    "#%mprun\n",
    "%memit\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #Parse XML [F:\\stackoverflow.com-Posts\\Posts.xml]Done, total [1001] records!\n",
    "    #Retrieved 1001 records in 0.1178131103515625 Seconds\n",
    "    #clean data in 2.963881731033325 Seconds\n",
    "    #idlist, cleaned_data = loadAndCleanData(force_reload=True, n_jobs=4, \n",
    "    #                                        force_reload_file=r\"F:\\stackoverflow.com-Posts\\Posts.xml\",\n",
    "    #                                        force_reload_count=10000)\n",
    "    idlist, cleaned_data = loadAndCleanData() \n",
    "    #cleaned_data_dict=dict(zip(idlist, cleaned_data))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    #tokenized data in 13.708552837371826 Seconds\n",
    "    #vectorize done in 0.02807474136352539 Seconds\n",
    "    #tokenized_data, vecterized_data, vectorizer = tokenizeAndVectorieData(cleaned_data,\n",
    "    #                                                                      force_reprocess = True,\n",
    "    #                                                                      n_jobs=4,\n",
    "    #                                                                      min_df=5, #5\n",
    "    #                                                                      max_df=0.6, #0.6\n",
    "    #                                                                      max_features=20000)\n",
    "    tokenized_data, vecterized_data, vectorizer = vectorizData(cleaned_data, n_jobs=4,min_df=1,max_df=1.0,max_features=20000)\n",
    "    #if tokenized_data:\n",
    "    #    tokenized_data_dict=dict(zip(idlist, tokenized_data))\n",
    "    #vecterized_data_dict=dict(zip(idlist, vecterized_data))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    #n_jobs=-1, Trained LDA in 303.9054253101349 Seconds\n",
    "    #n_jobs=1, Trained LDA in 146.29173946380615 Seconds\n",
    "    start_time = time.time()\n",
    "    lda = utilLDA.trainLDA(vecterized_data, n_topics=10, n_jobs=1)\n",
    "    try:\n",
    "        utilTools.dillDump(\"./lda.pkl\", lda)\n",
    "    except:\n",
    "        pass        \n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    utilLDA.print_top_words(lda, feature_names, 20)\n",
    "    end_time = time.time()\n",
    "    print(\"Trained LDA in {} Seconds\".format(end_time - start_time))\n",
    "    \n",
    "    #get document_topic_distribution\n",
    "    doc_topic_distr = lda.transform(vecterized_data[:15])\n",
    "    utilLDA.visDocTopicDist(doc_topic_distr)\n",
    "    print(doc_topic_distr.shape)\n",
    "    print(idlist[0])\n",
    "    print(doc_topic_distr[0])\n",
    "    \n",
    "    #utilLDA.visLDA(lda, vecterized_data, vectorizer, utilTools.getIP(), 8889)\n",
    "    utilLDA.visLDA(lda, vecterized_data, vectorizer, \"127.0.0.1\",8889)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
