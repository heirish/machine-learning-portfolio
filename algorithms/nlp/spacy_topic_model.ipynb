{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ElasticSearchClass' from 'E:\\\\my_study_place\\\\python\\\\jupyter\\\\spacy\\\\ElasticSearchClass.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin \n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stopwords \n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "import hdbscan\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "import string\n",
    "import time\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [16,9]\n",
    "\n",
    "import ElasticSearchClass\n",
    "import importlib\n",
    "importlib.reload(ElasticSearchClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load20NewsGroups():\n",
    "    # #############################################################################\n",
    "    # Load some categories from the training set\n",
    "    categories = [\n",
    "        'alt.atheism',\n",
    "        'talk.religion.misc',\n",
    "        'comp.graphics',\n",
    "        'sci.space',\n",
    "     ]\n",
    "    dataset = fetch_20newsgroups(subset='all', categories=categories,\n",
    "                             shuffle=True, random_state=42)\n",
    "    print(\"%d documents\" % len(dataset.data))\n",
    "    print(\"%d categories\" % len(dataset.target_names))\n",
    "    return dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object scan at 0x000002659A2661A8>\n",
      "Retrieved 10001 records in 112.29061961174011 Seconds\n"
     ]
    }
   ],
   "source": [
    "def loadStackoverflowFromES():\n",
    "    esUtil = ElasticSearchClass.ElasticSearchClass(\"192.168.18.187\", 9201)\n",
    "    dsl = '''\n",
    "    {\n",
    "    \"_source\":[\"title\", \"body\"],\n",
    "    \"query\":{\n",
    "        \"bool\":{\n",
    "            \"must\":{\n",
    "                \"match\":{\"posttypeid\":1}}\n",
    "            }\n",
    "        },\n",
    "    \"size\":1\n",
    "    }\n",
    "    '''\n",
    "    res = esUtil.search(indexName=\"posts\", body=dsl)\n",
    "    for doc in res['hits']['hits']:\n",
    "         print(\"%s) %s\" % (doc['_id'], doc['_source']))\n",
    "    \n",
    "def iterLoadStackoverflowFromES():\n",
    "    esUtil = ElasticSearchClass.ElasticSearchClass(\"192.168.18.187\", 9201)\n",
    "    dsl = '''\n",
    "    {\n",
    "    \"_source\":[\"body\"],\n",
    "    \"query\":{\n",
    "        \"bool\":{\n",
    "            \"must\":{\n",
    "                \"match\":{\"posttypeid\":1}}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    '''\n",
    "    res = esUtil.scrollSearch(indexName=\"posts\", body=dsl)\n",
    "    print(res)\n",
    "    count = 0\n",
    "    data = []\n",
    "    for doc in res:\n",
    "        if count > 10000:\n",
    "            break\n",
    "        count += 1\n",
    "        data.append([doc['_id'], doc['_source']['body']])\n",
    "        #print(doc['_id'], doc['_source']['body'])\n",
    "    return data\n",
    "            \n",
    "#loadStackoverflowFromES()\n",
    "start_time = time.time()\n",
    "data = iterLoadStackoverflowFromES()\n",
    "end_time = time.time()\n",
    "print(\"Retrieved {} records in {} Seconds\".format(len(data), end_time - start_time))\n",
    "#Retrieved 101 records in 1.3270199298858643 Seconds\n",
    "#10001 records in 246.03912162780762 Seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########create data clearner\n",
    "\n",
    "#Custom transformer using spaCy \n",
    "class CleanTextTransformer(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        return [cleanText(text) for text in X]\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "# A custom function to clean the text before sending it into the vectorizer\n",
    "def cleanText(text):\n",
    "    # get rid of newlines\n",
    "    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    # replace twitter @mentions\n",
    "    mentionFinder = re.compile(r\"@[a-z0-9_]{1,15}\", re.IGNORECASE)\n",
    "    text = mentionFinder.sub(\"@MENTION\", text)\n",
    "    # replace HTML symbols\n",
    "    text = text.replace(\"&amp;\", \"and\").replace(\"&gt;\", \">\").replace(\"&lt;\", \"<\")\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############create tokenizer\n",
    "\n",
    "#Create spacy tokenizer that parses a sentence and generates tokens\n",
    "#these can also be replaced by word vectors \n",
    "# List of symbols we don't care about\n",
    "punctuations = \" \".join(string.punctuation).split(\" \") + [\"-----\", \"---\", \"...\", \"“\", \"”\", \"'ve\", \"--\", \"//\", \"div\"]\n",
    "parser = spacy.load('en')\n",
    "def tokenizeText(sentence):\n",
    "    tokens = parser(sentence)\n",
    "    tokens = [tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_ for tok in tokens]\n",
    "    tokens = [tok for tok in tokens if (tok not in stopwords and tok not in punctuations)]  \n",
    "    # remove large strings of whitespace\n",
    "    while \"\" in tokens:\n",
    "        tokens.remove(\"\")\n",
    "    while \" \" in tokens:\n",
    "        tokens.remove(\" \")\n",
    "    while \"\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\")\n",
    "    while \"\\n\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\\n\")\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########Create preprocess pipline and run\n",
    "def preProcessData(X_train, max_features=None):\n",
    "    #create vectorizer object to generate feature vectors, we will use custom spacy’s tokenizer\n",
    "    #vectorizer = TfidfVectorizer(tokenizer = tokenizeText)\n",
    "    #svd = TruncatedSVD(2)\n",
    "    #normalizer = Normalizer(copy=False)\n",
    "    vectorizer = CountVectorizer(tokenizer = tokenizeText, max_features=max_features)\n",
    "    start_time = time.time()\n",
    "    pipe_preprocess = Pipeline([(\"cleaner\", CleanTextTransformer()),\n",
    "                 (\"vectorizer\", vectorizer)])\n",
    "    X_train_preprocess = pipe_preprocess.fit_transform(X_train)\n",
    "    end_time = time.time()\n",
    "    print(\"Preprocess done in {} Seconds\".format(end_time - start_time))\n",
    "    return X_train_preprocess, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3387 documents\n",
      "4 categories\n",
      "Preprocess done in 241.15877604484558 Seconds\n"
     ]
    }
   ],
   "source": [
    "X_train = load20NewsGroups()\n",
    "X_train_preprocess, vectorizer = preProcessData(X_train, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "tokens=parser(\"123 test\")\n",
    "for tok in tokens:\n",
    "    print(tok.lemma_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['471814', 'First Some Background (incase it helps):\\nMy application is a Web-based framework recently upgraded to v3.5 of the .Net Framework but does not use a Master Pages / User Controls system.  It\\'s more akin to the MVC pattern (although much older) and outputs pure HTML down the response stream from Templates.  The Python expressions allow some rules and template variations to be achieved.\\nThe old way\\nWhen embedding the IronPython 1.x engine in C#, we were able to do code such as:\\nPythonEngine pe = new PythonEngine();\\nAssembly a = Assembly.LoadFile(\"path to assembly\");\\npe.LoadAssembly(a);\\npe.Import(\"Script\");\\n\\nthere is no Import() method in ipy 2.0 and the ImportModule() method doesn\\'t seem to work the same way.  The Import() alleviated the need to put a line in every python script we write, such as:\\nfrom MyAssembly import MyClass\\n\\nthe fact that MyClass is full of static methods, means that calls to MyClass.MyMethod() work really well.  I can\\'t just instansiate an object and assign it to a variable in scope as the assembly that MyClass is contained in is dynamically loaded at runtime.  \\nNow to the issue\\nI have sorted out all the other parts of the integration of IronPython 2.0 but would prefer not to require my implementers to type \"from MyAssembly import MyClass\" at the top of every script they write (just seems silly when it was not necessary in ipy 1.x) and likely to be a support issue for a while too.\\nAnd finally the question\\nHas anyone had this issue and resolved it? Am I doing things the wrong way for the DLR? or am I missing something obvious?\\nI\\'m not sure of the detail required for someone to help, but I hope this is enough.\\n']\n",
      "[\"I'm looking for advice on how to give a junior developer a chance to gain experience on a big project with tight deadlines without hurting the timeline. If we all know it will take a little longer with the developer because of less experience, then what is the best way to give them a portion of the project and spread the risk? Giving these developers the ability to learn on the job on real projects instead of handing down maintenance work all the time is important to me, and I want to find a way to make it work.\\n\", 'First Some Background (incase it helps):\\nMy application is a Web-based framework recently upgraded to v3.5 of the .Net Framework but does not use a Master Pages / User Controls system.  It\\'s more akin to the MVC pattern (although much older) and outputs pure HTML down the response stream from Templates.  The Python expressions allow some rules and template variations to be achieved.\\nThe old way\\nWhen embedding the IronPython 1.x engine in C#, we were able to do code such as:\\nPythonEngine pe = new PythonEngine();\\nAssembly a = Assembly.LoadFile(\"path to assembly\");\\npe.LoadAssembly(a);\\npe.Import(\"Script\");\\n\\nthere is no Import() method in ipy 2.0 and the ImportModule() method doesn\\'t seem to work the same way.  The Import() alleviated the need to put a line in every python script we write, such as:\\nfrom MyAssembly import MyClass\\n\\nthe fact that MyClass is full of static methods, means that calls to MyClass.MyMethod() work really well.  I can\\'t just instansiate an object and assign it to a variable in scope as the assembly that MyClass is contained in is dynamically loaded at runtime.  \\nNow to the issue\\nI have sorted out all the other parts of the integration of IronPython 2.0 but would prefer not to require my implementers to type \"from MyAssembly import MyClass\" at the top of every script they write (just seems silly when it was not necessary in ipy 1.x) and likely to be a support issue for a while too.\\nAnd finally the question\\nHas anyone had this issue and resolved it? Am I doing things the wrong way for the DLR? or am I missing something obvious?\\nI\\'m not sure of the detail required for someone to help, but I hope this is enough.\\n']\n",
      "Preprocess done in 306.1574866771698 Seconds\n"
     ]
    }
   ],
   "source": [
    "print(data[1])\n",
    "X_train = [row[1] for row in data]\n",
    "print(X_train[:2])\n",
    "X_train_preprocess, vectorizer = preProcessData(X_train, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='online', learning_offset=50.0,\n",
       "             max_doc_update_iter=100, max_iter=50, mean_change_tol=0.001,\n",
       "             n_jobs=1, n_topics=4, perp_tol=0.1, random_state=125,\n",
       "             topic_word_prior=None, total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "n_topics = 4\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=50,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=125)\n",
    "lda.fit(X_train_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "use user page table datum server database like want way select work sql need query try form create view control\n",
      "Topic #1:\n",
      "class public object new method return string use set property code type value void // function error like null try\n",
      "Topic #2:\n",
      "0 1 text function image 2 // var code x like array div use int value html 3 return work\n",
      "Topic #3:\n",
      "use file like application project work code run way good need know want window just try look make time write\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "print_top_words(lda, feature_names, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program_Files\\Anaconda3\\lib\\site-packages\\_pytest\\fixtures.py:834: DeprecationWarning: The `convert` argument is deprecated in favor of `converter`.  It will be removed after 2019/01.\n",
      "  params = attr.ib(convert=attr.converters.optional(tuple))\n",
      "D:\\Program_Files\\Anaconda3\\lib\\site-packages\\_pytest\\fixtures.py:836: DeprecationWarning: The `convert` argument is deprecated in favor of `converter`.  It will be removed after 2019/01.\n",
      "  ids = attr.ib(default=None, convert=_ensure_immutable_ids)\n",
      "D:\\Program_Files\\Anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:387: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n",
      "  topic_term_dists = topic_term_dists.ix[topic_order]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: if you're in the IPython notebook, pyLDAvis.show() is not the best command\n",
      "      to use. Consider using pyLDAvis.display(), or pyLDAvis.enable_notebook().\n",
      "      See more information at http://pyLDAvis.github.io/quickstart.html .\n",
      "\n",
      "You must interrupt the kernel to end this command\n",
      "\n",
      "Serving to http://127.0.0.1:8889/    [Ctrl-C to exit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [14/Feb/2018 17:24:08] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Feb/2018 17:24:08] \"GET /LDAvis.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Feb/2018 17:24:09] \"GET /d3.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Feb/2018 17:24:09] \"GET /LDAvis.js HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "data = pyLDAvis.sklearn.prepare(lda, X_train_preprocess, vectorizer)\n",
    "pyLDAvis.show(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
