{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This's a test for spacy, add a Capital word, 试几个中文\n",
      "This\n",
      "'s\n",
      "a\n",
      "test\n",
      "for\n",
      "spacy\n",
      ",\n",
      "add\n",
      "a\n",
      "Capital\n",
      "word\n",
      ",\n",
      "试几个中文\n"
     ]
    }
   ],
   "source": [
    "#word tokenize test\n",
    "nlp = spacy.load('en')\n",
    "doc1 = nlp(u\"This's a test for spacy, add a Capital word, 试几个中文\")\n",
    "print(doc1)\n",
    "for token in doc1:\n",
    "    print(token)\n",
    "#只做了分词，没有转换大小写, 没有云掉puncutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test for space sentences.\n",
      "OK, try this: when will you back to your hometown?\n",
      "This have no space with the last sentence.再来一句中文. 你能认出来么, 这里会被分成几句\n"
     ]
    }
   ],
   "source": [
    "#sentence tokenize test\n",
    "nlp = spacy.load('en')\n",
    "doc2 = nlp(u\"This is a test for space sentences. OK, try this: when will you back to your hometown? This have no space with the last sentence.再来一句中文. 你能认出来么, 这里会被分成几句\")\n",
    "for sent in doc2.sents:\n",
    "    print(sent)\n",
    "#句与句之间要有空格，.?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windows 10646332009431770838 windows\n",
      "Authentication 9067479397929045832 authentication\n",
      "is 10382539506755952630 be\n",
      "not 447765159362469301 not\n",
      "enabled 1080083029942854337 enable\n",
      "so 9781598966686434415 so\n",
      "I 561228191312463089 -PRON-\n",
      "can 6635067063807956629 can\n",
      "not 447765159362469301 not\n",
      "login 7612924703817238883 login\n",
      ". 12646065887601541794 .\n",
      "Or 3740602843040177340 or\n",
      "is 10382539506755952630 be\n",
      "there 2112642640949226496 there\n",
      "a 11901859001352538922 a\n",
      "better 5711639017775284443 good\n",
      "solution 8054865091517299512 solution\n",
      "exists 2808794269826886559 exist\n",
      "? 8205403955989537350 ?\n"
     ]
    }
   ],
   "source": [
    "#Lemmatize Test 词干化\n",
    "doc3 = nlp(u\"Windows Authentication is not enabled so I cannot login. Or is there a better solution exists?\")\n",
    "for token in doc3:\n",
    "    print(token, token.lemma, token.lemma_)\n",
    "#转换成小写\n",
    "#is-->be\n",
    "#better-->good\n",
    "#exists-->exist\n",
    "#没有去掉puncutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windows 95 PROPN\n",
      "Authentication 95 PROPN\n",
      "is 99 VERB\n",
      "not 85 ADV\n",
      "enabled 99 VERB\n",
      "so 84 ADP\n",
      "I 94 PRON\n",
      "can 99 VERB\n",
      "not 85 ADV\n",
      "login 99 VERB\n",
      ". 96 PUNCT\n",
      "Or 88 CCONJ\n",
      "is 99 VERB\n",
      "there 85 ADV\n",
      "a 89 DET\n",
      "better 83 ADJ\n",
      "solution 91 NOUN\n",
      "exists 99 VERB\n",
      "? 96 PUNCT\n"
     ]
    }
   ],
   "source": [
    "#Pos Tagging Test\n",
    "for token in doc3:\n",
    "    print(token, token.pos, token.pos_)\n",
    "#词性\n",
    "#可以用来去除puncutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andrew Yan-Tak Ng 378 PERSON\n",
      "Chinese 379 NORP\n",
      "1976 388 DATE\n",
      "Chinese 379 NORP\n",
      "American 379 NORP\n",
      "Baidu 381 ORG\n",
      "Artificial Intelligence Group 381 ORG\n",
      "Stanford University 381 ORG\n",
      "Coursera 382 GPE\n"
     ]
    }
   ],
   "source": [
    "#Name entity recognizer test: 命名实体识别\n",
    "doc4=nlp(u\"Andrew Yan-Tak Ng (Chinese: 吳恩達; born 1976) is a Chinese American computer scientist. He is the former chief scientist at Baidu, where he led the company's Artificial Intelligence Group. He is an adjunct professor (formerly associate professor) at Stanford University. Ng is also the co-founder and chairman of Coursera, an online education platform\")\n",
    "for ent in doc4.ents:\n",
    "    print(ent, ent.label, ent.label_)\n",
    "\n",
    "#PERSON:人名\n",
    "#NORP:民族，宗教或政组织\n",
    "#Date:日期\n",
    "#ORG:组织\n",
    "#GPE:国家，城市， 州\n",
    "#可以用来识别日志中的实例，想办法自定义识别出IP, timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andrew Yan-Tak Ng\n",
      "a Chinese American computer scientist\n",
      "He\n",
      "the former chief scientist\n",
      "Baidu\n",
      "he\n",
      "the company's Artificial Intelligence Group\n",
      "He\n",
      "an adjunct professor\n",
      "formerly associate professor\n",
      "Stanford University\n",
      "Ng\n",
      "the co-founder and chairman\n",
      "Coursera\n",
      "an online education platform\n"
     ]
    }
   ],
   "source": [
    "#Noun Chunk Test: 名词短语提取\n",
    "for np in doc4.noun_chunks:\n",
    "    print(np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.040077213\n",
      "0.16391583\n",
      "0.6350539\n"
     ]
    }
   ],
   "source": [
    "# Word Vector Test: 其于词向量计算两个单词的相似度\n",
    "Andrew = doc4[0] #这里都己经表示成了向量\n",
    "computer = doc4[17]\n",
    "scientist =doc4[18]\n",
    "professor=\"professor\" #所以这样是不行的\n",
    "print(Andrew.similarity(scientist))\n",
    "print(Andrew.similarity(computer))\n",
    "print(computer.similarity(scientist))\n",
    "#print(Andrew.similarity(professor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'en_core_web_sm'\n",
      "Processing 2 texts\n",
      "Net income\tMONEY\t$9.4 million\n",
      "the prior year\tMONEY\t$2.7 million\n",
      "Revenue   \tMONEY\ttwelve billion dollars\n",
      "a loss    \tMONEY\t1b\n"
     ]
    }
   ],
   "source": [
    "## 提取实体关系\n",
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "import spacy\n",
    "\n",
    "\n",
    "TEXTS = [\n",
    "    'Net income was $9.4 million compared to the prior year of $2.7 million.',\n",
    "    'Revenue exceeded twelve billion dollars, with a loss of $1b.',\n",
    "]\n",
    "\n",
    "\n",
    "@plac.annotations(\n",
    "    model=(\"Model to load (needs parser and NER)\", \"positional\", None, str))\n",
    "def main(model='en_core_web_sm'):\n",
    "    nlp = spacy.load(model)\n",
    "    print(\"Loaded model '%s'\" % model)\n",
    "    print(\"Processing %d texts\" % len(TEXTS))\n",
    "\n",
    "    for text in TEXTS:\n",
    "        doc = nlp(text)\n",
    "        relations = extract_currency_relations(doc)\n",
    "        for r1, r2 in relations:\n",
    "            print('{:<10}\\t{}\\t{}'.format(r1.text, r2.ent_type_, r2.text))\n",
    "\n",
    "\n",
    "def extract_currency_relations(doc):\n",
    "    # merge entities and noun chunks into one token\n",
    "    spans = list(doc.ents) + list(doc.noun_chunks)\n",
    "    for span in spans:\n",
    "        span.merge()\n",
    "\n",
    "    relations = []\n",
    "    for money in filter(lambda w: w.ent_type_ == 'MONEY', doc):\n",
    "        if money.dep_ in ('attr', 'dobj'):\n",
    "            subject = [w for w in money.head.lefts if w.dep_ == 'nsubj']\n",
    "            if subject:\n",
    "                subject = subject[0]\n",
    "                relations.append((subject, money))\n",
    "        elif money.dep_ == 'pobj' and money.head.dep_ == 'prep':\n",
    "            relations.append((money.head.head, money))\n",
    "    return relations\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline ['tech_companies']\n",
      "Tokens ['Alphabet Inc.', 'is', 'the', 'company', 'behind', 'Google', '.']\n",
      "Doc has_tech_org True\n",
      "Token 0 is_tech_org True\n",
      "Token 1 is_tech_org False\n",
      "Entities [('Alphabet Inc.', 'ORG'), ('Google', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Example of a spaCy v2.0 pipeline component that sets entity annotations\n",
    "based on list of single or multiple-word company names. Companies are\n",
    "labelled as ORG and their spans are merged into one token. Additionally,\n",
    "._.has_tech_org and ._.is_tech_org is set on the Doc/Span and Token\n",
    "respectively.\n",
    "* Custom pipeline components: https://spacy.io//usage/processing-pipelines#custom-components\n",
    "Compatible with: spaCy v2.0.0+\n",
    "\"\"\"\n",
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "from spacy.lang.en import English\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "\n",
    "\n",
    "@plac.annotations(\n",
    "    text=(\"Text to process\", \"positional\", None, str),\n",
    "    companies=(\"Names of technology companies\", \"positional\", None, str))\n",
    "def main(text=\"Alphabet Inc. is the company behind Google.\", *companies):\n",
    "    # For simplicity, we start off with only the blank English Language class\n",
    "    # and no model or pre-defined pipeline loaded.\n",
    "    nlp = English()\n",
    "    if not companies:  # set default companies if none are set via args\n",
    "        companies = ['Alphabet Inc.', 'Google', 'Netflix', 'Apple']  # etc.\n",
    "    component = TechCompanyRecognizer(nlp, companies)  # initialise component\n",
    "    nlp.add_pipe(component, last=True)  # add last to the pipeline\n",
    "\n",
    "    doc = nlp(text)\n",
    "    print('Pipeline', nlp.pipe_names)  # pipeline contains component name\n",
    "    print('Tokens', [t.text for t in doc])  # company names from the list are merged\n",
    "    print('Doc has_tech_org', doc._.has_tech_org)  # Doc contains tech orgs\n",
    "    print('Token 0 is_tech_org', doc[0]._.is_tech_org)  # \"Alphabet Inc.\" is a tech org\n",
    "    print('Token 1 is_tech_org', doc[1]._.is_tech_org)  # \"is\" is not\n",
    "    print('Entities', [(e.text, e.label_) for e in doc.ents])  # all orgs are entities\n",
    "\n",
    "\n",
    "class TechCompanyRecognizer(object):\n",
    "    \"\"\"Example of a spaCy v2.0 pipeline component that sets entity annotations\n",
    "    based on list of single or multiple-word company names. Companies are\n",
    "    labelled as ORG and their spans are merged into one token. Additionally,\n",
    "    ._.has_tech_org and ._.is_tech_org is set on the Doc/Span and Token\n",
    "    respectively.\"\"\"\n",
    "    name = 'tech_companies'  # component name, will show up in the pipeline\n",
    "\n",
    "    def __init__(self, nlp, companies=tuple(), label='ORG'):\n",
    "        \"\"\"Initialise the pipeline component. The shared nlp instance is used\n",
    "        to initialise the matcher with the shared vocab, get the label ID and\n",
    "        generate Doc objects as phrase match patterns.\n",
    "        \"\"\"\n",
    "        self.label = nlp.vocab.strings[label]  # get entity label ID\n",
    "\n",
    "        # Set up the PhraseMatcher – it can now take Doc objects as patterns,\n",
    "        # so even if the list of companies is long, it's very efficient\n",
    "        patterns = [nlp(org) for org in companies]\n",
    "        self.matcher = PhraseMatcher(nlp.vocab)\n",
    "        self.matcher.add('TECH_ORGS', None, *patterns)\n",
    "\n",
    "        # Register attribute on the Token. We'll be overwriting this based on\n",
    "        # the matches, so we're only setting a default value, not a getter.\n",
    "        Token.set_extension('is_tech_org', default=False)\n",
    "\n",
    "        # Register attributes on Doc and Span via a getter that checks if one of\n",
    "        # the contained tokens is set to is_tech_org == True.\n",
    "        Doc.set_extension('has_tech_org', getter=self.has_tech_org)\n",
    "        Span.set_extension('has_tech_org', getter=self.has_tech_org)\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        \"\"\"Apply the pipeline component on a Doc object and modify it if matches\n",
    "        are found. Return the Doc, so it can be processed by the next component\n",
    "        in the pipeline, if available.\n",
    "        \"\"\"\n",
    "        matches = self.matcher(doc)\n",
    "        spans = []  # keep the spans for later so we can merge them afterwards\n",
    "        for _, start, end in matches:\n",
    "            # Generate Span representing the entity & set label\n",
    "            entity = Span(doc, start, end, label=self.label)\n",
    "            spans.append(entity)\n",
    "            # Set custom attribute on each token of the entity\n",
    "            for token in entity:\n",
    "                token._.set('is_tech_org', True)\n",
    "            # Overwrite doc.ents and add entity – be careful not to replace!\n",
    "            doc.ents = list(doc.ents) + [entity]\n",
    "        for span in spans:\n",
    "            # Iterate over all spans and merge them into one token. This is done\n",
    "            # after setting the entities – otherwise, it would cause mismatched\n",
    "            # indices!\n",
    "            span.merge()\n",
    "        return doc  # don't forget to return the Doc!\n",
    "\n",
    "    def has_tech_org(self, tokens):\n",
    "        \"\"\"Getter for Doc and Span attributes. Returns True if one of the tokens\n",
    "        is a tech org. Since the getter is only called when we access the\n",
    "        attribute, we can refer to the Token's 'is_tech_org' attribute here,\n",
    "        which is already set in the processing step.\"\"\"\n",
    "        return any([t._.get('is_tech_org') for t in tokens])\n",
    "\n",
    "main()\n",
    "    # Expected output:\n",
    "    # Pipeline ['tech_companies']\n",
    "    # Tokens ['Alphabet Inc.', 'is', 'the', 'company', 'behind', 'Google', '.']\n",
    "    # Doc has_tech_org True\n",
    "    # Token 0 is_tech_org True\n",
    "    # Token 1 is_tech_org False\n",
    "    # Entities [('Alphabet Inc.', 'ORG'), ('Google', 'ORG')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n",
      "{'ner': 19.558236360549927}\n",
      "{'ner': 14.745993072603596}\n",
      "{'ner': 11.542696918196498}\n",
      "{'ner': 14.38020922677226}\n",
      "{'ner': 9.85609424795561}\n",
      "{'ner': 3.5626718958740184}\n",
      "{'ner': 3.6819251688477435}\n",
      "{'ner': 6.9809978348852395}\n",
      "{'ner': 8.36336357941655}\n",
      "{'ner': 8.520106192316252}\n",
      "{'ner': 8.797412687945833}\n",
      "{'ner': 12.88454615854036}\n",
      "{'ner': 3.72887843955949}\n",
      "{'ner': 0.7986601515369851}\n",
      "{'ner': 2.5107453060453935}\n",
      "{'ner': 1.9720739553377198}\n",
      "{'ner': 1.0585482862800424}\n",
      "{'ner': 1.9833813895683194}\n",
      "{'ner': 0.9036725126217289}\n",
      "{'ner': 2.481782546589562}\n",
      "{'ner': 0.00012695567780440168}\n",
      "{'ner': 0.08297985243381768}\n",
      "{'ner': 0.4068829501889216}\n",
      "{'ner': 0.02961837654670605}\n",
      "{'ner': 1.9997692670523712}\n",
      "{'ner': 0.0002583125256980492}\n",
      "{'ner': 0.8201337977707732}\n",
      "{'ner': 1.801046368404278}\n",
      "{'ner': 5.517401668892613e-06}\n",
      "{'ner': 8.493352189590618e-12}\n",
      "{'ner': 0.0018518639206181102}\n",
      "{'ner': 0.13071620679875195}\n",
      "{'ner': 6.538074707778533e-14}\n",
      "{'ner': 0.0864296929161509}\n",
      "{'ner': 0.0027009884339357863}\n",
      "{'ner': 5.967756037891205e-23}\n",
      "{'ner': 0.013458572329575117}\n",
      "{'ner': 3.0495318268869426e-11}\n",
      "{'ner': 1.3395082118461013e-06}\n",
      "{'ner': 0.0014881805985172871}\n",
      "{'ner': 1.9616184286808773e-16}\n",
      "{'ner': 8.491021436600454e-18}\n",
      "{'ner': 3.097994522287596e-06}\n",
      "{'ner': 2.879051870812817e-08}\n",
      "{'ner': 2.424417836392715e-15}\n",
      "{'ner': 0.005872369130149505}\n",
      "{'ner': 5.464359028686923e-16}\n",
      "{'ner': 1.6871748886786558e-17}\n",
      "{'ner': 2.7015326850261583e-05}\n",
      "{'ner': 2.826780409343059e-10}\n",
      "{'ner': 7.030948185452503e-08}\n",
      "{'ner': 3.6395982294079795e-21}\n",
      "{'ner': 4.967234331100402e-21}\n",
      "{'ner': 3.9257515069608645e-20}\n",
      "{'ner': 2.1232768876786406e-09}\n",
      "{'ner': 1.4485601664371967e-15}\n",
      "{'ner': 3.876230093937645e-13}\n",
      "{'ner': 0.0031985011883080006}\n",
      "{'ner': 2.1087954908522503e-10}\n",
      "{'ner': 1.1263273599489966e-18}\n",
      "{'ner': 6.897514127046271e-17}\n",
      "{'ner': 1.9198019318677672e-15}\n",
      "{'ner': 8.635443987232567e-15}\n",
      "{'ner': 4.86590679521352e-12}\n",
      "{'ner': 1.2196973184859126e-11}\n",
      "{'ner': 2.462786047622849e-20}\n",
      "{'ner': 1.5558769133148968e-21}\n",
      "{'ner': 3.931249992222917e-09}\n",
      "{'ner': 1.0794466673842597e-22}\n",
      "{'ner': 5.819008067782898e-08}\n",
      "{'ner': 0.0005554888104271559}\n",
      "{'ner': 1.786830239360909e-07}\n",
      "{'ner': 1.7768380641937256}\n",
      "{'ner': 1.8066112841983268e-11}\n",
      "{'ner': 1.1282832150548212e-15}\n",
      "{'ner': 3.485732119951119e-14}\n",
      "{'ner': 1.3894116348832626e-21}\n",
      "{'ner': 0.0004417439922873091}\n",
      "{'ner': 0.00016392962426180267}\n",
      "{'ner': 2.606496881725107e-16}\n",
      "{'ner': 9.660434970126458e-15}\n",
      "{'ner': 3.08144363745334e-17}\n",
      "{'ner': 8.795024221430222e-14}\n",
      "{'ner': 4.1615851850028953e-19}\n",
      "{'ner': 1.2906637282557058e-17}\n",
      "{'ner': 1.9459301503683538e-17}\n",
      "{'ner': 1.6308120218831822e-12}\n",
      "{'ner': 2.2245464314579476e-19}\n",
      "{'ner': 1.2172098666661963e-22}\n",
      "{'ner': 9.327408274452696e-12}\n",
      "{'ner': 9.311435804622184e-18}\n",
      "{'ner': 4.649235883028753e-25}\n",
      "{'ner': 1.3453927751357467e-07}\n",
      "{'ner': 6.248016680217206e-20}\n",
      "{'ner': 7.365560734883858e-32}\n",
      "{'ner': 9.356040386876569e-09}\n",
      "{'ner': 2.954996716205222e-21}\n",
      "{'ner': 8.38924575790523e-30}\n",
      "{'ner': 7.966517339739109e-27}\n",
      "{'ner': 2.575091675561564e-13}\n",
      "Entities [('Shaka Khan', 'PERSON')]\n",
      "Tokens [('Who', '', 2), ('is', '', 2), ('Shaka', 'PERSON', 3), ('Khan', 'PERSON', 1), ('?', '', 2)]\n",
      "Entities [('London', 'LOC'), ('Berlin', 'LOC')]\n",
      "Tokens [('I', '', 2), ('like', '', 2), ('London', 'LOC', 3), ('and', '', 2), ('Berlin', 'LOC', 3), ('.', '', 2)]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Example of training spaCy's named entity recognizer, starting off with an\n",
    "existing model or a blank model.\n",
    "For more details, see the documentation:\n",
    "* Training: https://spacy.io/usage/training\n",
    "* NER: https://spacy.io/usage/linguistic-features#named-entities\n",
    "Compatible with: spaCy v2.0.0+\n",
    "\"\"\"\n",
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "\n",
    "\n",
    "# training data\n",
    "TRAIN_DATA = [\n",
    "    ('Who is Shaka Khan?', {\n",
    "        'entities': [(7, 17, 'PERSON')]\n",
    "    }),\n",
    "    ('I like London and Berlin.', {\n",
    "        'entities': [(7, 13, 'LOC'), (18, 24, 'LOC')]\n",
    "    })\n",
    "]\n",
    "\n",
    "\n",
    "@plac.annotations(\n",
    "    model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
    "    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
    "    n_iter=(\"Number of training iterations\", \"option\", \"n\", int))\n",
    "def main(model=None, output_dir=None, n_iter=100):\n",
    "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank('en')  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    # otherwise, get it so we can add labels\n",
    "    else:\n",
    "        ner = nlp.get_pipe('ner')\n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            for text, annotations in TRAIN_DATA:\n",
    "                nlp.update(\n",
    "                    [text],  # batch of texts\n",
    "                    [annotations],  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    sgd=optimizer,  # callable to update weights\n",
    "                    losses=losses)\n",
    "            print(losses)\n",
    "\n",
    "    # test the trained model\n",
    "    for text, _ in TRAIN_DATA:\n",
    "        doc = nlp(text)\n",
    "        print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n",
    "        print('Tokens', [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        for text, _ in TRAIN_DATA:\n",
    "            doc = nlp2(text)\n",
    "            print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n",
    "            print('Tokens', [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "main()\n",
    "    # Expected output:\n",
    "    # Entities [('Shaka Khan', 'PERSON')]\n",
    "    # Tokens [('Who', '', 2), ('is', '', 2), ('Shaka', 'PERSON', 3),\n",
    "    # ('Khan', 'PERSON', 1), ('?', '', 2)]\n",
    "    # Entities [('London', 'LOC'), ('Berlin', 'LOC')]\n",
    "    # Tokens [('I', '', 2), ('like', '', 2), ('London', 'LOC', 3),\n",
    "    # ('and', '', 2), ('Berlin', 'LOC', 3), ('.', '', 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#训练自己的新的entity https://github.com/explosion/spacy/blob/master/examples/training/train_new_entity_type.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参考"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
