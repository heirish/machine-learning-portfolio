{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ElasticSearchClass' from 'E:\\\\my_study_place\\\\python\\\\jupyter\\\\spacy\\\\ElasticSearchClass.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin \n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stopwords \n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "import hdbscan\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "import string\n",
    "import time\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [16,9]\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import ElasticSearchClass\n",
    "import importlib\n",
    "importlib.reload(ElasticSearchClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A custom function to clean the text before sending it into the vectorizer\n",
    "def cleanText(text):\n",
    "   #some data will raise NotImplementedError: subclasses of ParserBase must override error()\n",
    "    try:\n",
    "        bs = BeautifulSoup(text, \"html.parser\")\n",
    "        #code = [s.extract() for s in bs('code')]\n",
    "        # replace other HTML symbols\n",
    "        text = bs.get_text()\n",
    "    except Exception as e:\n",
    "        #print(e)\n",
    "        pass\n",
    "    text = text.lower()\n",
    "    # get rid of newlines\n",
    "    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    # replace @xxxx with @mentions\n",
    "    mentionFinder = re.compile(r\"@[a-z0-9_]{1,15}\", re.IGNORECASE)\n",
    "    text = mentionFinder.sub(\"@mention\", text)\n",
    "    # here we don't need @mention\n",
    "    text = re.sub(\"@mention\", '', text).strip()\n",
    "    # delete numbers\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object scan at 0x0000026B4B43BBA0>\n",
      "Retrieved 1092 records in 1.1353392601013184 Seconds\n"
     ]
    }
   ],
   "source": [
    "def iterLoadStackoverflowFromES():\n",
    "    esUtil = ElasticSearchClass.ElasticSearchClass(\"192.168.18.187\", 9201)\n",
    "    dsl = '''\n",
    "    {\n",
    "    \"_source\":[\"body\"],\n",
    "    \"query\":{\n",
    "        \"bool\":{\n",
    "            \"must\":{\n",
    "                \"match\":{\"posttypeid\":1}}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    '''\n",
    "    res = esUtil.scrollSearch(indexName=\"stackoverflow\", body=dsl)\n",
    "    print(res)\n",
    "    count = 0\n",
    "    data = []\n",
    "    for doc in res:\n",
    "        if count > 10000:\n",
    "            break\n",
    "        count += 1\n",
    "        data.append([doc['_id'], doc['_source']['body']])\n",
    "        #print(doc['_id'], doc['_source']['body'])\n",
    "    return data\n",
    "            \n",
    "#loadStackoverflowFromES()\n",
    "start_time = time.time()\n",
    "data = iterLoadStackoverflowFromES()\n",
    "end_time = time.time()\n",
    "print(\"Retrieved {} records in {} Seconds\".format(len(data), end_time - start_time))\n",
    "#Retrieved 101 records in 1.3270199298858643 Seconds\n",
    "#10001 records in 246.03912162780762 Seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "Preprocess done in 0.4301903247833252 Seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%time\n",
    "X_train = [row[1] for row in data]\n",
    "start_time = time.time()\n",
    "X_train_preprocess = [cleanText(txt) for txt in X_train]\n",
    "#print(X_train_preprocess[:10])\n",
    "end_time = time.time()\n",
    "print(\"Preprocess done in {} Seconds\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "if \"__name__ = __main__\":\n",
    "    start_time = time.time()\n",
    "    X_train_preprocess = [Parallel(n_jobs=8)(delayed(cleanText)(txt)for txt in X_train)]\n",
    "    print(X_train_preprocess[:10])\n",
    "    end_time = time.time()\n",
    "    print(\"Preprocess done in {} Seconds\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing \n",
    "%time\n",
    "start_time = time.time()\n",
    "pool = multiprocessing.Pool(processes=2)\n",
    "results = pool.map(cleanText, X_train)\n",
    "#print(results)\n",
    "end_time = time.time()\n",
    "print(\"Preprocess done in {} Seconds\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "1500000 [1499999]\n",
      "can process list of length 1500000, time 0.461627721786499\n"
     ]
    }
   ],
   "source": [
    "import mytest\n",
    "import importlib\n",
    "importlib.reload(mytest)\n",
    "    \n",
    "#for multiprocessing 'i' format requires -2147483648 <= number <= 2147483647 \n",
    "import time\n",
    "if __name__ == \"__main__\":\n",
    "    N = 1500000\n",
    "    n_chunks=10\n",
    "    start_time=time.time()\n",
    "    X=list(range(N))\n",
    "    mytest.chunkParallel(X, 10)\n",
    "    end_time=time.time()\n",
    "    print(\"can process list of length {}, time {}\".format(N, end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program_Files\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: This is a warning message.\n",
      "D:\\Program_Files\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead\n",
      "WARNING:root:[+] This is a warn message.\n",
      "INFO:root:[+] This is a info message.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python  \n",
    "# -*- coding: utf8 -*- \n",
    "     \n",
    "import logging \n",
    "import warnings  \n",
    "     \n",
    "     \n",
    "logging.basicConfig(level=logging.INFO) \n",
    " \n",
    " \n",
    "def filterwarn(): \n",
    "    # warnings.simplefilter('ignore', UserWarning) \n",
    "    # advance warnings flter function \n",
    "    warnings.filterwarnings('ignore', '.*warn.*', UserWarning, 'module') \n",
    " \n",
    " \n",
    "def main(): \n",
    "    filterwarn() \n",
    "    # compare the two following items. \n",
    "    warnings.warn(\"This is a warning message.\") \n",
    "    logging.warn(\"[+] This is a warn message.\") \n",
    " \n",
    "    logging.info(\"[+] This is a info message.\") \n",
    " \n",
    "if __name__ == \"__main__\": \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"projectName:\"\"line-test\"\" AND body:\"\"Hello\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projectName%3A%22avatar-chat%22%20AND%20%28body%3A%22FAILURE_DB%22%29\n"
     ]
    }
   ],
   "source": [
    "from urllib import parse\n",
    "\n",
    "#这个是js的结果\n",
    "# encodeURIComponent('中国')\n",
    "# \"%E4%B8%AD%E5%9B%BD\"\n",
    "#jsRet='%E4%B8%AD%E5%9B%BD'\n",
    "#print(parse.unquote(jsRet))       #输出：中国\n",
    "#print(jsRet==parse.quote('中国'))  #输出：True\n",
    "print(parse.quote(\"projectName:\\\"avatar-chat\\\" AND (body:\\\"FAILURE_DB\\\")\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils import utilIdentifier\n",
    "import importlib\n",
    "importlib.reload(utilIdentifier)\n",
    "\n",
    "def identifyText(text):\n",
    "    text = utilIdentifier.identifyIP(text)\n",
    "    text = utilIdentifier.identifyDatetime(text)\n",
    "    text = utilIdentifier.identifyUri(text)\n",
    "    text = utilIdentifier.identifyNumber(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= IP_TYPE IP_TYPE DATETIME_TYPE [warn]: dest=URL_TYPE #0 failed to flush the buffer IP_TYPE . retry_time=NUM_TYPE \n",
      "\n",
      "IP_TYPE IP_TYPE DATETIME_TYPE [warn]: dest=URL_TYPE #0 failed to flush the buffer IP_TYPE . retry_time=NUM_TYPE next_retry_seconds=DATETIME_TYPE chunk=56745277a4532957f8c4fe9e070b75d1 error_class=NoMethodError error=undefined method `has_key?' for #<String:0x007ff21c6d91b8>\n",
      "IP_TYPE NNP ip_type\n",
      "IP_TYPE NNP ip_type\n",
      "DATETIME_TYPE NNP datetime_type\n",
      "[ -LRB- [\n",
      "warn VBP warn\n",
      "] -RRB- ]\n",
      ": : :\n",
      "dest JJS d\n",
      "= SYM =\n",
      "URL_TYPE NNS url_type\n",
      "# $ #\n",
      "0 CD 0\n",
      "failed VBD fail\n",
      "to TO to\n",
      "flush VB flush\n",
      "the DT the\n",
      "buffer NN buffer\n",
      "IP_TYPE NN ip_type\n",
      ". . .\n",
      "retry_time NN retry_time\n",
      "= . =\n",
      "NUM_TYPE NNP num_type\n",
      "next_retry_seconds NNS next_retry_second\n",
      "= SYM =\n",
      "DATETIME_TYPE NNS datetime_type\n",
      "chunk=56745277a4532957f8c4fe9e070b75d1 VBP chunk=56745277a4532957f8c4fe9e070b75d1\n",
      "error_class NN error_class\n",
      "= SYM =\n",
      "NoMethodError NN nomethoderror\n",
      "error NN error\n",
      "= SYM =\n",
      "undefined JJ undefined\n",
      "method NN method\n",
      "` '' `\n",
      "has_key NN has_key\n",
      "? . ?\n",
      "' '' '\n",
      "for IN for\n",
      "# $ #\n",
      "< XX <\n",
      "String:0x007ff21c6d91b8 XX string:0x007ff21c6d91b8\n",
      "> XX >\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import string\n",
    "\n",
    "text = \"192.168.1.100 fe80::20c:29ff:fe75:f519/64 2018-03-13 16:27:24 +0900 [warn]: dest=http://github.com/obdg/plda.git #0 failed to flush the buffer fe80::20c:29ff:fe75:f519/64 . retry_time=2 next_retry_seconds=2018-03-13 16:27:54 +0900 chunk=\"\"56745277a4532957f8c4fe9e070b75d1\"\" error_class=NoMethodError error=\"\"undefined method `has_key?' for #<String:0x007ff21c6d91b8>\"\n",
    "text = identifyText(text)\n",
    "print(text)\n",
    "#https://spacy.io/usage/processing-pipelines\n",
    "#https://github.com/explosion/spaCy/issues/1837\n",
    "parser = spacy.load('en', disable=['parser', 'ner'])\n",
    "tokens = parser(text)\n",
    "for tok in tokens:\n",
    "    print(tok, tok.tag_, tok.lemma_)\n",
    "    \n",
    "#for ent in tokens.ents:\n",
    "#    print(ent, ent.label, ent.lable_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014 CD 2014\n",
      "- SYM -\n",
      "07 CD 07\n",
      "- SYM -\n",
      "12 CD 12\n",
      "05:21 CD 05:21\n",
      "INFO NNP info\n",
      "https NN https\n",
      "source=192.168.32.10 PRP source=192.168.32.10\n"
     ]
    }
   ],
   "source": [
    "text= \"2014-07-12 05:21 INFO https source=192.168.32.10\"\n",
    "tokens = parser(text)\n",
    "for tok in tokens:\n",
    "    print(tok, tok.tag_, tok.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/64 \n",
      "True\n",
      "\n",
      "\n",
      "=2 \n",
      "True\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for match in re.finditer(\"[/\\:\\-\\,\\s\\_\\+\\@=]\\d+[\\s\\,\\.]\",\"buffer fe80::20c:29ff:fe75:f519/64 . retry_time=2 \"):\n",
    "    print(match.group())\n",
    "    print(match.group()[1:].rstrip(\",. \").isnumeric())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
